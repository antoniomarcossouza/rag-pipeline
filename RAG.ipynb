{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66b3fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from utils.arxiv import create_arxiv_vectorstore\n",
    "from utils.sql import DatabaseConnection\n",
    "\n",
    "POSTGRES_USER = \"username\"\n",
    "POSTGRES_PASSWORD = \"password\"\n",
    "POSTGRES_HOST = \"localhost\"\n",
    "POSTGRES_PORT = \"5432\"\n",
    "POSTGRES_DB = \"vectordb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7efd6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DatabaseConnection(\n",
    "    db_name=POSTGRES_DB,\n",
    "    user=POSTGRES_USER,\n",
    "    password=POSTGRES_PASSWORD,\n",
    "    host=POSTGRES_HOST,\n",
    "    port=POSTGRES_PORT,\n",
    ")\n",
    "\n",
    "vectorstore = await create_arxiv_vectorstore(db.langchain_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1197df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: list[Document]) -> str:\n",
    "    \"\"\"\n",
    "    Formata uma lista de documentos em uma string estruturada.\n",
    "\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    docs : list[Document]\n",
    "        Lista de objetos do tipo `Document` a serem formatados.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    str\n",
    "        Uma string contendo o conteúdo dos documentos formatados com seus IDs e separadores.\n",
    "    \"\"\"\n",
    "\n",
    "    context = f\"{'-'*90}\\n\\n\"\n",
    "    for doc in docs:\n",
    "        context += f\"DOC ID: {doc.metadata['doc_id']}\\n\\n\"\n",
    "        context += doc.page_content\n",
    "        context += f\"\\n{'-'*90}\\n\\n\"\n",
    "\n",
    "    return context[:-2]\n",
    "\n",
    "\n",
    "def ollama_llm(question: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Gera uma resposta para uma pergunta com base em um contexto fornecido.\n",
    "\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    question : str\n",
    "        A pergunta a ser respondida.\n",
    "    context : str\n",
    "        O contexto no qual a resposta será baseada.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    str\n",
    "        A resposta gerada pelo modelo de linguagem,\n",
    "        em português, citando o documento de origem.\n",
    "    \"\"\"\n",
    "\n",
    "    formatted_prompt = f\"\"\"\n",
    "Responda a pergunta a seguir com base no contexto fornecido.\n",
    "Se a resposta não estiver no contexto, diga que não sabe.\n",
    "Responda em português e cite o documento de onde a informação foi retirada.\n",
    "\n",
    "Pergunta: {question}\\n\\n\n",
    "Contexto:\\n{context}\n",
    "    \"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3.2:1b\",\n",
    "        messages=[{\"role\": \"user\", \"content\": formatted_prompt}],\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def rag(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Executa o processo de RAG para responder a uma pergunta.\n",
    "\n",
    "    Este método utiliza um vetor de armazenamento para recuperar\n",
    "    documentos relevantes com base na pergunta fornecida, formata\n",
    "    os documentos recuperados e utiliza um modelo de linguagem\n",
    "    para gerar uma resposta com base no contexto.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    str\n",
    "        A resposta gerada pelo modelo de linguagem\n",
    "        com base nos documentos recuperados.\n",
    "    \"\"\"\n",
    "\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "\n",
    "    return ollama_llm(question, formatted_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "657f6678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: O.stream computing paradigm é um modelo que permite o tratamento de dados em tempo real, distribuído e escalável, utilizando máquinas virtuais (VMs) ou servidores em nuvem. Ele tem como características principais a escalabilidade, flexibilidade e capacidade de processar grandes volumes de dados rapidamente.\n",
      "\n",
      "Documento de origem: \n",
      "- DOC ID 750\n",
      "- DOC ID 224\n"
     ]
    }
   ],
   "source": [
    "print(rag(\"What is the stream computing paradigm?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
